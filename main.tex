\documentclass{beamer}

\usepackage[style=numeric,maxnames=2, defernumbers=true]{biblatex}
\usepackage{minted}
\usemintedstyle{colorful}

\usepackage{tikz}

\definecolor{lightgray}{gray}{0.9}
\definecolor{arbgrey}{RGB}{84, 84, 84}
\definecolor{arbred}{RGB}{153, 67, 25}
\definecolor{arborange}{RGB}{201, 104, 44}
\definecolor{mediumslateblue}{HTML}{7B68EE}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{\node[shape=circle,fill,inner sep=2pt] (char) {\textcolor{white}{#1}};}} % chktex 36

\usepackage[orientation=portrait,size=a1,scale=1.0]{beamerposter}
\usetheme{JuelichPoster}

\usetikzlibrary{positioning}

\ExecuteBibliographyOptions{%
  sorting=nyt,
  bibwarn=true,
  isbn=false,
  url=true%
}
\addbibresource{references.bib}

\setbeamertemplate{partner1}{\includegraphics{img/cscs}}
\setbeamertemplate{partner2}{\includegraphics{img/HBP_logo}}

\begin{document}

\newcommand{\nmlcc}{\texttt{nmlcc}}

\begin{frame}[t, fragile]
  \frametitle{\includegraphics[width=0.66\linewidth]{img/arbor-lines-proto-colour-full}}
  \framesubtitle{Generating High Performance Simulations from a Portable Data Format using Arbor\\
    \tiny{T. Hater, B. Huisman, L. P. L. Landsmeer (Forschungszentrum JÃ¼lich)}}
  \begin{columns}[onlytextwidth,T]
    \begin{column}{0.76\textwidth}
      Computational neuroscience is experiencing a steady growth in available
      simulation tools applicable to morphologically detailed cell descriptions
      \cite{eden, neurogpu, carl, brian, arb, nrn}. However, the development of
      models that are actually portable between simulators lags behind. NeuroML2
      is one of the few comprehensive approaches in this area, but its reference
      implementation lacks in performance and scalability to larger simulations
      \cite{nml2}. Our goal is to enable Arbor, a performance-portable library
      for simulating morphologically detailed neurons to consume NeuronML2
      models. We present \nmlcc, a tool to generate optimised, full scale
      simulations from a description in NeuroML2. It produces bespoke dynamics
      tailored to the input, resulting in performance metrics comparable to
      hand-optimized code. Through Arbor, the generated simulation package is
      able to utilize modern hardware, including large-scale GPU clusters,
      scaling to millions of cells \cite{arb}. As a case study, we show how a
      single cell simulation based on \cite{Hay} was ported to Arbor using
      \nmlcc. Finally, we analyse the runtime performance of the produced model.

      This poster and all associated data and scripts can be found at the URL on
      the right.
    \end{column}
    \begin{column}{0.22\textwidth}
      \vspace*{-1ex}
      \begin{block}{Where to find\dots}
        \begin{description}
          \item[Arbor] \href{https://arbor-sim.github.io}{arbor-sim.github.io}
          \item[nmlcc] \href{https://github.com/thorstenhater/nmlcc}{github.com/thorstenhater/nmlcc}
          \item[This] \href{https://github.com/thorstenhater/nmlcc}{github.com/thorstenhater/hbpsc-2022}
          \item[Contact] \href{mailto:t.hater@fz-juelich.de}{t.hater@fz-juelich.de}
          \item[License] GPL-3
        \end{description}
      \end{block}
    \end{column}
  \end{columns}
  % problem statement
  \vspace*{1ex}
  \textcolor{arbgrey}{\rule{\textwidth}{0.5ex}}
  \vspace*{-1ex}
  \begin{columns}
    \begin{column}{0.55\textwidth}
      \textbf{Dynamic Compilation}

      NeuroML2 (NML) constitutes a data language for describing a full neuronal
      network, including network connectivity, cell morphologies,
      parametrisation, and ion channel dynamics. NML is defined as a library of
      LEMS specifications, which can be dynamically extended while building a
      simulation. This constrains our choice of implementation, since we cannot
      deliver a pre-built set of ion channels as an addition to Arbor, but must
      be able to synthesise these on demand. On the other hand, having the full
      simulation available for introspection enables a plethora of
      optimisations.
    \end{column}
    \begin{column}{0.43\textwidth}
\begin{minted}[fontsize=\small]{xml}
<ionChannelHH id="kChan" conductance="10pS" species="k">
  <gateHHrates id="n" instances="4">
    <forwardRate type="HHExpLinearRate" rate="1per_ms" midpoint="-55mV" scale="10mV"/>
    <reverseRate type="HHExpRate" rate="1.25per_ms" midpoint="-65mV" scale="-80mV"/>
  </gateHHrates>
</ionChannelHH>
\end{minted}
    \end{column}
  \end{columns}
  % Data flow
  \vspace*{1ex}
  \textcolor{arbred}{\rule{\textwidth}{0.5ex}}
  \vspace*{-1ex}
  \begin{columns}
    \begin{column}{0.44\textwidth}
      \begin{tikzpicture}
        % input
        \node[draw, rectangle, rounded corners, ultra thick, color=black!80, text width=8cm] (nml) {\verb!<neuroml>!\\\verb!  ...!\\\verb!</neuroml>!};
        % nmlcc products
        \node[draw, rectangle, rounded corners, ultra thick, color=black!80, text width=5cm, below left=2.25cm and -3.5cm of nml] (mrf) {\verb!mrf/morph-0.nml!\\\verb!mrf/morph-1.nml!};
        \node[draw, rectangle, rounded corners, ultra thick, color=black!80, text width=4.5cm, right=1cm of mrf] (acc) {\verb!acc/cell-0.acc!\\\verb!acc/cell-1.acc!};
        \node[draw, rectangle, rounded corners, ultra thick, color=black!80, text width=5cm, right=1cm of acc]   (sim) {\verb!main.py!\\\verb!import arbor as A!};
        \node[draw, rectangle, rounded corners, ultra thick, color=black!80, text width=4cm, left=1cm of mrf] (cat) {\verb!cat/hh.mod!\\\verb!cat/leak.mod!};
        \node[draw, rectangle, rounded corners, ultra thick, color=black!80, text width=6cm, below=1cm of cat] (cso) {\verb!local-catalogue.so!};
        % connections from nml to outputs
        \node[below=1.5cm of nml] (h0) {};
        \draw[ultra thick, minimum size=0pt] (nml.south) -- (h0.center) node[right, pos=0.5]{\verb!nmlcc bundle!};
        \draw[->, >=stealth, ultra thick, minimum size=0pt] (h0.center) -| (acc.north);
        \draw[->, >=stealth, ultra thick, minimum size=0pt] (h0.center) -| (sim.north);
        \draw[->, >=stealth, ultra thick, minimum size=0pt] (h0.center) -| (mrf.north);
        \draw[->, >=stealth, ultra thick, minimum size=0pt] (h0.center) -| (cat.north);
        % connection from outputs to sim
        \node[below=1cm of sim] (h1) {};
        \draw[ultra thick, ->, >=stealth] (acc.south) |- ([xshift=-10pt, yshift=10pt]h1.center)  -| ([xshift=-10pt]sim.south);
        \draw[ultra thick, ->, >=stealth] (mrf.south) |- (h1.center) -| (sim.south);
        \draw[ultra thick, ->, >=stealth] (cat.south) -- (cso.north);
        \draw[ultra thick, ->, >=stealth] ([yshift=0pt]cso.east) -| ([xshift=10pt]sim.south);
      \end{tikzpicture}
    \end{column}
    \begin{column}{0.54\textwidth}
      \textbf{Dataflow}

      \nmlcc consumes NML2 descriptions at the level of individual ion
      channels, cells (comprising a morphology, ion channels, and parameter
      assignments), and full networks, which include cells, their connectivity,
      and network parameters. This is achieved by translating components into
      Arbor's native format and connecting them in a Python script that provides
      a runnable simulation.
      \begin{description}
        \item[Ion channel] Transformed to NMODL, translated to C++, and compiled to a
              shared library.
        \item[Morphology] Extracted to standalone NML2 morphology files, which Arbor can
              load natively.
        \item[Assignments] Stored in Arbor's cable cell data format for passive parameters and ion channel mappings.
        \item[Connectivity] Stored inside the Python script as dictionaries of cell id, source, and target.
        \item[Stimuli] Added by the Python script, stored as a list of target location, amplitude, start time, and duration.
      \end{description}
    \end{column}
  \end{columns}
  % Data flow
  \vspace*{1ex}
  \textcolor{arbgrey}{\rule{\textwidth}{0.5ex}}
  \vspace*{-1ex}
  \begin{columns}[t]
    \begin{column}{0.49\textwidth}
      \textbf{Optimisations for Ion Channels}

      Arbor's model for building simulations is highly flexible: cell types and
      their ion channel dynamics are given at runtime. Ion channels can be
      loaded from shared libraries at start-up. However, this flexibility comes
      at the cost of missing performance optimisations. Since \nmlcc can
      inspect the full simulation, it has extensive potential for optimisation,
      mainly when dealing with ion channel descriptions. These are one of the
      two main cost centres in a simulation, the other is solving the cable
      equation. We perform two central high-level transformations:
      Specialisation and Combination. Both designed to expose more optimisations
      to the later stages of building the shared libraries. We exploit that NML2
      provides assignments of the form \verb!(region, channel, parameters)!
      where
      \begin{description}
        \item[\texttt{region}] sub-set of the morphology's segments
        \item[\texttt{channel}] the ion channel name
        \item[\texttt{parameters}] a list of key-value pairs to be set on the channel
      \end{description}
    \end{column}
    \begin{column}{0.49\textwidth}
      \emph{Specialisation} Since NML2 defines ion channels as parametrised
      templates, we can generate one copy of each assigned channel per region
      where the parameters have been fixed to their concrete values for this
      region. This allows for replacing memory accesses by constants and folding
      multiple constants into one. Then, all operations on these constant values
      are evaluated and conditionals are replaced by the appropriate branch
      where possible.\\[1.5ex]
      \emph{Combination} Using the same information, we can also combine all ion
      channels added to the same region in to a single one, dubbed a
      `super-mechanism'. As Arbor's ion channels are implemented as a set of
      callbacks on a standardised set
      of parameters, this reduces the amount of functions called and data moved.\\[1.5ex]

      \emph{General Optimisations} When generating NMODL from NML2 we also apply
      a series of guidelines we found helpful to achieve good performance. These
      mainly minimise the amount of data movement;\@ see Arbor's NMODL
      documentation.
    \end{column}
  \end{columns}
  % Evaluation
  \vspace*{1ex}
  \textcolor{arborange}{\rule{\textwidth}{0.5ex}}
  \vspace*{-1ex}
  \begin{columns}
    \begin{column}{0.73\textwidth}
      \begin{columns}[t]
        \begin{column}[t]{0.45\textwidth}
          \textbf{Performance}
          \vspace*{1ex}

          \includegraphics[width=\textwidth]{./benchmark/barchart_cropped.pdf}

          Performance comparison across simulators and setups on simulation of
          figure 5a from Hay \emph{et al.}. By default $\Delta t = 0.025ms$ is used,
          except for the CVODE case, which leverages NEURON's adaptive time-stepping.
        \end{column}
        \begin{column}[t]{0.53\textwidth}
          \textbf{Benchmark}

          We compare performance using the Layer 5 pyramidal cell model of Hay
          \emph{et al.} across NeuroML supporting simulators NEURON, EDEN and Arbor.
          The original cell model is tuned for simulation with NEURON, yet the
          provided NeuroML description lends itself for ready conversion to EDEN,
          Arbor, and again NEURON via jnml. We selected figure 4b among the
          available simulation setups from the paper repository (left)~\cite{HayNML}. This
          example has the highest compute density. All simulations are
          performed on the JUWELS cluster, on a single Intel Xeon Platinum 8168 CPU
          running at 2.7 GHz.

          \textbf{Results}

          Among fixed time stepping methods, Arbor is the clear winner for both jnml
          converted model and our \nmlcc compiler output. NEURONs adaptive
          timestepping method leads to a significant speedup in comparison to fixed
          timestepping, but is still slower than Arbor's fixed timestepping method.
          This is in part due to the transient heavy simulation setup, for single
          spikes in long simulation times CVODE would outperform Arbor by a small
          amount. We think that this is a boundary case that should not be observed
          in realistic simulation scenarios; especially in network simulations, as
          CVODE does not allow for incoming spikes. We show that the merging of
          channel mechanisms yields another 10\% speedup over regular translation.
          EDEN allows for direct consumption of NML2 models, and while it is
          significantly faster than NEURON, it does not achieve performance
          comparable to Arbor.

          Importantly, the \nmlcc method is fully automatic, yet still manages to
          outperform \texttt{jnml} Arbor conversion and the original optimized model
          implementation. Arbor \nmlcc faithfully reproduces expected critical firing
          behaviour of figure 5a (right).
        \end{column}
      \end{columns}
      % Development and Future Work.
      \vspace*{1ex}
      \textcolor{arbred}{\rule{\textwidth}{0.59ex}}
      \vspace*{-1ex}
      \begin{columns}[t]
        \begin{column}[t]{0.49\textwidth}
          \textbf{Future Work}

          While preparing this presentation we identified a series of possible
          enhancements to \nmlcc in addition to our preexisting roadmap.

          \emph{Using C++ instead of Python} to improve runtime performance for the
          generated simulation -- especially as we store connectivity data inside --
          we consider generating C++ code instead of Python.\\[1.5ex]
          \emph{Fine-grained regions} currently we rely on identical region tags for
          identifying targets for combination and specialisation. To maximise the
          gains from these optimisations, we should synthesise regions from the
          existing tagged subsets such that each branch is covered by exactly one
          tag. Thus a single, completely specialised mechanism will be generated per
          tag.
        \end{column}
        \begin{column}[t]{0.49\textwidth}
          \textbf{Development Methodolgy}

          \nmlcc is written in Rust, a modern high-performance, memory safe
          language that offers features convenient for writing compiler-like
          software -- such as pattern matching, automated serialisation, and
          algebraic data types -- in addition to a rich packaging ecosystem, which
          makes installation on enduser's device trivial. The internal data model is
          automatically generated from the schemea files and base libraries included
          with NML2. This two-phase approach allows us to rapidly adapt to changes
          in NML2. \nmlcc is published on GitHub under the GPL-3 open-source
          license, which is compatible with NML2. We use the platforms CI service to
          ensure each update matches a series of known good outputs as well as
          conforms to accepted coding standards.
        \end{column}
      \end{columns}
    \end{column}

    \begin{column}{0.24\textwidth}
      \begin{figure}[h]
        \includegraphics[width=\textwidth]{./benchmark/5a_cropped.pdf}
      \end{figure}
    \end{column}
  \end{columns}

  % References
  \vspace*{1ex}
  \textcolor{arbgrey}{\rule{\textwidth}{0.5ex}}
  \vspace*{-1ex}
  \begin{columns}
    \begin{column}{0.49\textwidth}
      \textbf{References}

      \printbibliography[type=article]
    \end{column}
    \begin{column}{0.49\textwidth}
      \printbibliography[nottype=article]

      \textbf{Acknowledgements}

      We are grateful to P.~Gleeson for many insightful discussions on the
      internals of NML2.

      This research has received funding from the European Union's Horizon 2020
      Framework Programme for Research and Innovation under the Specific Grant
      Agreements No.\,720270 (HBP SGA1), No.\,785907 (HBP SGA2), and No.\,945539
      (HBP SGA3).
    \end{column}
  \end{columns}
\end{frame}
\end{document}
